# Running and fine-tuning Open Source LLMs on GKE 

This repository contains samples of running and fine-tuning Open Source LLMs on GKE.

1. The first section is about running Open Source LLMs using two different frameworks:
    - vLLM
    - TGI
2. Fine Tuning Open Source LLMs on GKE using TGI




# Resources:

- https://medium.com/@tech-gumptions/transformer-architecture-simplified-3fb501d461c8

- https://medium.com/@crismunozv/using-fine-tuned-llm-with-vllm-ee34e7db5495

- https://medium.com/google-cloud/serving-open-source-llms-on-gke-using-vllm-framework-5e522b3679ee

- https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4

- https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/tutorials-and-examples/genAI-LLM/serving-llama2-70b-on-l4-gpus

- https://github.com/GoogleCloudPlatform/container-engine-accelerators/blob/master/daemonset.yaml

- https://medium.com/@rohit.k/tgi-vs-vllm-making-informed-choices-for-llm-deployment-37c56d7ff705

- https://github.com/llm-on-gke/vllm-inference
