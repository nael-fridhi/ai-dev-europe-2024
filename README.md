# Running and fine-tuning Open Source LLMs on GKE 

This repository contains samples of running and fine-tuning Open Source LLMs on GKE.

1. The first section is about running Open Source LLMs using two different frameworks:
    - vLLM
    - TGI
2. Fine Tuning Open Source LLMs on GKE using TGI




# Resources:

- https://medium.com/@tech-gumptions/transformer-architecture-simplified-3fb501d461c8

- https://medium.com/google-cloud/serving-open-source-llms-on-gke-using-vllm-framework-5e522b3679ee

- https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/tutorials-and-examples/genAI-LLM/finetuning-llama-7b-on-l4

- https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/tutorials-and-examples/genAI-LLM/serving-llama2-70b-on-l4-gpus

